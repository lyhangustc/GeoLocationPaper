
\begin{teaserfigure}
	\includegraphics[width=\textwidth]{figures/new_teaser_pdf}
	\caption{(a) The overhead image is captured by an aerial device in a low altitude. (b) The point cloud is scanned by a laser device on the ground. We extract the roof contours (in different colors) according to the altitude histogram of points (c) The contours are matched with the overhead image respectively to achieve an initialization for optimizing the global matrix. (d) The camera pose is estimated after the iterative global matrix optimization. We project the contours on the overhead image to show the results.}
	\label{fig:teaser}
\end{teaserfigure}


\maketitle



\section{Introduction}
%
As a key technique to image-based navigation, augmented reality (AR) and 3D reconstruction, geo-localization has drawn massive attentions in the literature. When presenting a geo-localization problem, an image or a frame of video is often used as the query data, a 3D model is needed to provide a global coordinate, a sensor prior is optionally employed and the camera pose with respect to the global coordinate system is to be estimated. 

In this work, we estimate the camera pose using an overhead image captured by a low-altitude aerial device as query and a corresponding building point cloud as 3D model. The building point clouds we deal with are scanned by a laser device, and only facades of buildings are able to be scanned. Comparing to existing methods using images captured on the ground~\cite{instant} or in high altitude~\cite{FDCM}, we face challenges that we are not able to take advantages of vanishing points in overhead image and suffer from more critical perspective effect in low-altitude image. 

There are two key observations that vertical facades of a point cloud correspond to edges of building roofs in the overhead image and that roofs at different altitudes are of different scales in the image, which inspire us to treat this geo-localization problem as a combination of a multi-layer shape matching problem and a global optimizing procedure. 
%利用这个方法，我们可以。。。。。
%
\begin{figure}[t]
	\centering
	%\vspace{2.0cm}
	\includegraphics[width=0.5\textwidth]{figures/details_pdf}
	\caption{Finding paired 3D and 2D feature points: for a 3D feature point, we project it on the overhead image (magenta points) using current project matrix and search in its $k\times k$ neighborhood for a corner (cyan points). These 3D feature points and corresponding corners form pairs of feature points for next iteration of calculating project matrix.}
	\label{fig:overview}
\end{figure}
\section{Our Approach}
Given a building point cloud, we first project points upward along the vertical direction to several specific altitudes determined by altitude histogram of points. And then we fit those points into line segments in each altitude and treat the sets of line segments as contours of building roofs (Figure 1b). 
%Given a building point cloud, we first extract contours of building roofs in different altitudes according to the altitude histogram of points, where each contour is fit into a set of line segments (Figure 1b). 
And then the contours are matched with the edge map of the overhead image respectively using a shape matching technique of \cite{FDCM}. A local project matrix is achieved for each contour after shape matching (Figure 1c). Note that we need a global project matrix instead of local ones to estimate the camera pose. So we treat the results of shape matching as the initialization of following global optimizing procedure. 

To calculate the global project matrix between the whole point cloud and the overhead image, we need a series of paired 3D feature points and corresponding 2D feature points. We take the intersections of neighboring line segments of the contours as 3D feature points. And then we employ an iterative algorithm to optimize the global project matrix by alternatively finding corresponding 2D feature points and calculating the project matrix, where we utilize the results of shape matching as initial project matrix.

To be more specific, we apply two steps in every iteration alternatively until the convergence. (1)~As shown in Figure~2, for a 3D feature point we project it on the overhead image using current project matrix (magenta points) and search in its $k\times k$ neighborhood for a corner with maximal corner response (cyan points). These 3D feature points and corresponding corners form paired feature points for the next step of calculating project matrix.
%Once a set of paired 3D and 2D feature points is achieved, we randomly select a subset of these feature point pairs to calculate a global matrix. 
(2) With a set of paired 3D and 2D feature points, we can optimize a global project matrix by minimizing the average of distances between projected contours and the edges of the overhead image, which can be accelerated by distance transformation. And the newly calculated project matrix is used for the next iteration of finding paired feature points.

With the optimized global project matrix and pre-calibrated intrinsic matrix of the camera, we can finally estimate the 6 degree-of-freedom camera pose of the overhead image in point cloud coordinate system.
% Be more specific?
\section{Experiments and Future Work}
%We tested the proposed approach on a newly collected dataset, which contains several low-altitude overhead images and corresponding building point clouds. Figure 3 shows one of the results and a comparison of some details of the result, where we project roof contours on the overhead image. As we can see, the result after the global optimization is not much more accuracy than that before the global optimization.
We tested the proposed approach on a newly collected dataset, which contains several low-altitude overhead images and corresponding building point clouds. Figure 3 shows the results of our experiments, where we project roof contours on the overhead image. 
Since most of the consuming time is spent in the shape matching stage and the optimizing stage is fast, we intend to implement a faster version of the algorithm and extend it to video-based problems. 

\begin{figure}[b]
	\centering
	%\vspace{2.0cm}
	\includegraphics[width=0.5\textwidth]{figures/results_pdf}
	\caption{(a) The low-altitude overhead images. (b) Building point cloud with contours we extract. (c) Results of our experiments. We project the contours on the overhead image to show the results.}
	\label{fig:comparison}
\end{figure}
