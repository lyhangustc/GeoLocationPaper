@inproceedings{boulch2012fast,
	title={Fast and robust normal estimation for point clouds with sharp features},
	author={Boulch, Alexandre and Marlet, Renaud},
	booktitle={Computer graphics forum},
	volume={31},
	number={5},
	pages={1765--1774},
	year={2012},
	organization={Wiley Online Library}
}

@article{Barrow,
title = {to be checked}
}
@ARTICLE{Zamir2014,
author={A. R. Zamir and M. Shah},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Image Geo-Localization Based on MultipleNearest Neighbor Feature Matching Using Generalized Graphs},
year={2014},
volume={36},
number={8},
pages={1546-1558},
keywords={feature extraction;graph theory;image matching;image retrieval;radial basis function networks;G-RBF;GMCP-based feature matching;Gaussian radial basis function;generalized minimum clique graphs;global features;image geo-localization;image matching;local features extraction;multiple nearest neighbor feature matching;nearest neighbor retrieval;query feature;query image;robust distance function;street view images;Context;Equations;Feature extraction;Image color analysis;Image edge detection;Robustness;Visualization;Generalized Minimum Clique Problem (GMCP);Geo-location;feature correspondence;feature matching;generalized graphs;generalized minimum spanning tree (GMST);image localization;multiple nearest neighbor feature matching},
doi={10.1109/TPAMI.2014.2299799},
ISSN={0162-8828},
month={Aug},}
@inproceedings{BMVC.18.84,
   title = {An Image-Based System for Urban Navigation},
   author = {Robertsone, D. and Cipolla, R.},
   year = {2004},
   pages = {84.1-84.10},
   booktitle = {Proceedings of the British Machine Vision Conference},
   publisher = {BMVA Press},
   editors = {Hoppe, Andreas and Barman, Sarah and Ellis, Tim},
   isbn = {1-901725-25-1},
   note = {doi:10.5244/C.18.84}
}
@article{Taneja2015,
abstract = {We propose a method to detect changes in the geometry of a city using panoramic images captured by a car driving around the city. The proposed method can be used to significantly optimize the process of updating the 3D model of an urban environment that is changing over time, by restricting this process to only those areas where changes are detected. With this application in mind, we designed our algorithm to specifically detect only structural changes in the environment, ignoring any changes in its appearance, and ignoring also all the changes which are not relevant for update purposes such as cars, people etc. The approach also accounts for the challenges involved in a large scale application of change detection, such as inaccuracies in the input geometry, errors in the geo-location data of the images as well as the limited amount of information due to sparse imagery. We evaluated our approach on a small scale setup using high resolution, densely captured images and a large scale setup covering an entire city using instead the more realistic scenario of low resolution, sparsely captured images. A quantitative evaluation was also conducted for the large scale setup consisting of 14,000 images.},
author = {Taneja, Aparna and Ballan, Luca and Pollefeys, Marc},
doi = {10.1109/TPAMI.2015.2404834},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2015{\_}Taneja, Ballan, Pollefeys{\_}Geometric Change Detection in Urban Environments Using Images.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Change detection,Streetview image application,image registration},
number = {11},
pages = {2193--2206},
pmid = {26440261},
title = {{Geometric Change Detection in Urban Environments Using Images}},
volume = {37},
year = {2015}
}
@article{Bettio2015,
abstract = {We propose an approach for improving the digitization of shape and color of 3D artworks in a cluttered environment using 3D laser scanning and flash photography. To separate clutter from acquired material, semiautomated methods are employed to generate masks used to segment the range maps and the color photographs. This approach allows the removal of unwanted 3D and color data prior to the integration of acquired data in a 3D model. Sharp shadows generated by flash acquisition are easily handled by this masking process, and color deviations introduced by the flash light are corrected at the color blending step by taking into account the geometry of the object. The approach has been evaluated in a large-scale acquisition campaign of the Mont'e Prama complex. This site contains an extraordinary collection of stone fragments from the Nuragic era, which depict small models of prehistoric nuraghe (cone-shaped stone towers), as well as larger-than-life archers, warriors, and boxers. The acquisition campaign has covered 37 statues mounted on metallic supports. Color and shape were acquired at a resolution of 0.25mm, which resulted in more than 6,200 range maps (about 1.3G valid samples) and 3,817 photographs.},
author = {Bettio, Fabio and Computing, C R S Visual and Villanueva, Alberto Jaspe and Merella, Emilio and Marton, Fabio and Gobbetti, Enrico},
doi = {10.1145/2644823},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2015{\_}Bettio et al.{\_}Mont'e Scan Effective Shape and Color Digitization of Cluttered 3D Artworks.pdf:pdf},
issn = {15564711},
journal = {ACM Journal on Computing and Cultural Heritage},
keywords = {3D scanning,3d visualization,color acquisition,shape acquisition},
number = {1},
pages = {1--23},
title = {{Mont'e Scan : Effective Shape and Color Digitization of Cluttered 3D Artworks}},
volume = {8},
year = {2015}
}
@article{Lu2015,
author = {Lu, Guoyu and Yan, Yan and Ren, Li and Song, Jingkuan and Sebe, Nicu and Kambhamettu, Chandra},
doi = {10.1109/ICCV.2015.280},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2015{\_}Lu et al.{\_}Localize Me Anywhere , Anytime A Multi-task Point-Retrieval Approach.pdf:pdf},
isbn = {978-1-4673-8391-2},
journal = {Iccv 2015},
pages = {2434--2442},
title = {{Localize Me Anywhere , Anytime : A Multi-task Point-Retrieval Approach}},
year = {2015}
}
@article{Chang2015,
author = {Chang, Ming-ching},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2015{\_}Chang{\_}SEEING AS IT HAPPENS REAL TIME 3D VIDEO EVENT VISUALIZATION.pdf:pdf},
isbn = {9781479983391},
journal = {International Conference on Image Processing (ICIP)},
pages = {2875--2879},
title = {{SEEING AS IT HAPPENS : REAL TIME 3D VIDEO EVENT VISUALIZATION}},
year = {2015}
}
@article{Rodriguez2015,
abstract = {We introduce a novel approach for letting casual viewers explore detailed 3D models integrated with structured spatially associated descriptive information organized in a graph. Each node associates a subset of the 3D surface seen from a particular viewpoint to the related descriptive annotation, together with its author-defined importance. Graph edges describe, instead, the strength of the dependency relation between information nodes, allowing content authors to describe the preferred order of presentation of information. At run-time, users navigate inside the 3D scene using a camera controller, while adaptively receiving unobtrusive guidance towards interesting viewpoints and history- and location-dependent suggestions on important information, which is adaptively presented using 2D overlays displayed over the 3D scene. The capabilities of our approach are demonstrated in a real-world cultural heritage application involving the public presentation of sculptural complex on a large projection-based display. A user study has been performed in order to validate our approach.},
author = {Rodriguez, M. Balsa and Agus, M. and Marton, F. and Gobbetti, E.},
doi = {10.1111/cgf.12616},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2015{\_}Rodriguez et al.{\_}Adaptive Recommendations for Enhanced Non-linear Exploration of Annotated 3D Objects.pdf:pdf},
issn = {14678659},
journal = {Computer Graphics Forum},
keywords = {I.3.6 [Computer Graphics]: Methodology and Techniq,I.5.2 [Information Interfaces And Presentation (HC},
number = {3},
pages = {41--50},
title = {{Adaptive Recommendations for Enhanced Non-linear Exploration of Annotated 3D Objects}},
volume = {34},
year = {2015}
}
@article{Vidal2015,
author = {Vidal, Esteban and Piotto, Nicola and Cordara, Giovanni and Burgos, Francisco Moran},
doi = {10.1109/ICIP.2015.7351282},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2015{\_}Vidal et al.{\_}Automatic video to point cloud registration in a structure-from-motion framework.pdf:pdf},
isbn = {9781479983391},
issn = {15224880},
journal = {Proceedings - International Conference on Image Processing, ICIP},
keywords = {3D Reconstruction,Point Cloud Alignment,SfM,Video Registration},
pages = {2646--2650},
title = {{Automatic video to point cloud registration in a structure-from-motion framework}},
volume = {2015-Decem},
year = {2015}
}
@article{Iwaszczuk2014,
abstract = {{\textless}p{\textgreater}Thermal infrared imagery of urban areas became interesting for urban climate investigations and thermal building inspections. Using a flying platform such as UAV or a helicopter for the acquisition and combining the thermal data with the 3D building models via texturing delivers a valuable groundwork for large-area building inspections. However, such thermal textures are useful for further analysis if they are geometrically correctly extracted. This can be achieved with a good coregistrations between the 3D building models and thermal images, which cannot be achieved by direct georeferencing. Hence, this paper presents methodology for alignment of 3D building models and oblique TIR image sequences taken from a flying platform. In a single image line correspondences between model edges and image line segments are found using accumulator approach and based on these correspondences an optimal camera pose is calculated to ensure the best match between the projected model and the image structures. Among the sequence the linear features are tracked based on visibility prediction. The results of the proposed methodology are presented using a TIR image sequence taken from helicopter in a densely built-up urban area. The novelty of this work is given by employing the uncertainty of the 3D building models and by innovative tracking strategy based on a priori knowledge from the 3D building model and the visibility checking.{\textless}/p{\textgreater}},
author = {Iwaszczuk, D. and Stilla, U.},
doi = {10.5194/isprsannals-II-1-17-2014},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2014{\_}Iwaszczuk, Stilla{\_}Alignment of 3D Building Models and TIR Video Sequences with Line Tracking.pdf:pdf},
issn = {2194-9050},
journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
keywords = {aerial,building,image,infrared,model,registration,sequences,thermal},
number = {November},
pages = {17--24},
title = {{Alignment of 3D Building Models and TIR Video Sequences with Line Tracking}},
url = {http://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/II-1/17/2014/},
volume = {II-1},
year = {2014}
}
@article{ECDM,
author = {Zhang, Xi and Agam, Gady and Chen, Xin},
doi = {10.1109/CVPRW.2014.115},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2014{\_}Zhang, Agam, Chen{\_}Alignment of 3D Building Models with Satellite Images Using Extended Chamfer Matching.pdf:pdf},
isbn = {978-1-4799-4308-1},
issn = {21607516},
journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops},
pages = {746--753},
title = {{Alignment of 3D Building Models with Satellite Images Using Extended Chamfer Matching}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6910066},
year = {2014}
}
@article{Gerke2014,
abstract = {Automatic urban object detection from airborne remote sensing data is essential to process and efficiently interpret the vast amount of airborne imagery and Laserscanning (ALS) data available today. This paper combines ALS data and airborne imagery to exploit both: the good geometric quality of ALS and the spectral image information to detect the four classes buildings, trees, vegetated ground and sealed ground. A new segmentation approach is introduced which also makes use of geometric and spectral data during classification entity definition. Geometric, textural, low level and mid level image features are assigned to laser points which are quantified into voxels. The segment information is transferred to the voxels and those clusters of voxels form the entity to be classified. Two classification strategies are pursued: a supervised method, using Random Trees and an unsupervised approach, embedded in a Markov Random Field framework and using graph-cuts for energy optimization. A further contribution of this paper concerns the image-based point densification for building roofs which aims to mitigate the accuracy problems related to large ALS point spacing.Results for the ISPRS benchmark test data show that to rely on color information to separate vegetation from non-vegetation areas does mostly lead to good results, but in particular in shadow areas a confusion between classes might occur. The unsupervised classification strategy is especially sensitive in this respect. As far as the point cloud densification is concerned, we observe similar sensitivity with respect to color which makes some planes to be missed out, or false detections still remain. For planes where the densification is successful we see the expected enhancement of the outline. {\textcopyright} 2013 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).},
author = {Gerke, Markus and Xiao, Jing},
doi = {10.1016/j.isprsjprs.2013.10.011},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2014{\_}Gerke, Xiao{\_}Fusion of airborne laserscanning point clouds and images for supervised and unsupervised scene classification.pdf:pdf},
isbn = {0924-2716},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Classification,Segmentation,Supervised,Unsupervised,Visibility},
pages = {78--92},
publisher = {International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
title = {{Fusion of airborne laserscanning point clouds and images for supervised and unsupervised scene classification}},
url = {http://dx.doi.org/10.1016/j.isprsjprs.2013.10.011},
volume = {87},
year = {2014}
}
@article{Morago2014,
author = {Morago, Brittany and Bui, Giang and Duan, Ye},
doi = {10.1109/CVPRW.2014.113},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2014{\_}Morago, Bui, Duan{\_}Integrating LIDAR range scans and photographs with temporal changes.pdf:pdf},
isbn = {9781479943098},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
keywords = {2D-3D fusion,LIDAR range scan,contextual features,histogram of gradients,line segments,registration},
pages = {732--737},
title = {{Integrating LIDAR range scans and photographs with temporal changes}},
year = {2014}
}
@article{Kim2014,
author = {Kim, Hyojin and Correa, Carlos D. and Max, Nelson},
doi = {10.1109/ICCPHOT.2014.6831821},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2014{\_}Kim, Correa, Max{\_}Automatic registration of LiDAR and optical imagery using depth map stereo.pdf:pdf},
isbn = {9781479951888},
journal = {2014 IEEE International Conference on Computational Photography, ICCP 2014},
title = {{Automatic registration of LiDAR and optical imagery using depth map stereo}},
year = {2014}
}
@article{Grelsson2013,
abstract = {A method for online global pose estimation of aerial images by alignment with a georeferenced 3D model is presented. Motion stereo is used to reconstruct a dense local height patch from an image pair. The global pose is inferred from the 3D transform between the local height patch and the model. For efficiency, the sought 3D similarity transform is found by least-squares minimizations of three 2D subproblems. The method does not require any landmarks or reference points in the 3D model, but an approximate initialization of the global pose, in our case provided by onboard navigation sensors, is assumed. Real aerial images from helicopter and aircraft flights are used to evaluate the method. The results show that the accuracy of the position and orientation estimates is significantly improved compared to the initialization and our method is more robust than competing methods on similar datasets. The proposed matching error computed between the transformed patch and the map clearly indicates whether a reliable pose estimate has been obtained.},
author = {Grelsson, Bertil and Felsberg, Michael and Isaksson, Folke},
doi = {10.1109/WORV.2013.6521919},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2013{\_}Grelsson, Felsberg, Isaksson{\_}Efficient 7D aerial pose estimation.pdf:pdf},
isbn = {9781467356466},
journal = {2013 IEEE Workshop on Robot Vision, WORV 2013},
pages = {88--95},
title = {{Efficient 7D aerial pose estimation}},
year = {2013}
}
@article{Dellepiane2013,
abstract = {The management, processing and visualization of color information is a critical subject in the context of the acquisition and visualization of real objects. Especially in the context of Cultural Heritage, artifacts are so complex or hard-to-handle that the appearance information has to be extracted from a set of images. The images usually have to be registered to the 3D model of the objects, in order to transfer the needed information. Hence, the problem of image-to-geometry registration has been thoroughly studied by the Computer Graphics and Computer Vision community. Several methods have been proposed, but a fully automatic and generic solution is still missing. Moreover, small misalignments often lead to visible artifacts in the final colored 3D models. In this paper, we propose a method to refine the alignment of a group of images which has been already registered to a 3D model. Taking advantage of the overlapping among the images, and applying a statistical global method based on Mutual Information, the registration error is distributed among all the elements of the dataset. Hence, the quality of color projection is improved, especially when dealing with small details. The method was tested on a number of heterogeneous Cultural Heritage objects, bringing to a visible improvement in the rendering quality. The method is fully automatic, and it does not need powerful hardware or long processing time. Hence, it represents a valid solution for a wide application on CH artifacts.},
author = {Dellepiane, Matteo and Scopigno, Roberto},
doi = {10.1109/DigitalHeritage.2013.6743711},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2013{\_}Dellepiane, Scopigno{\_}Global refinement of image-to-geometry registration for color projection.pdf:pdf},
isbn = {9781479931699},
journal = {Proceedings of the DigitalHeritage 2013 - Federating the 19th Int'l VSMM, 10th Eurographics GCH, and 2nd UNESCO Memory of the World Conferences, Plus Special Sessions fromCAA, Arqueologica 2.0 et al.},
pages = {39--46},
title = {{Global refinement of image-to-geometry registration for color projection}},
volume = {1},
year = {2013}
}
@article{Crews2013,
author = {Karl, Ni and Nicholas, Armstrong{\-}Crews and Scott, Sawyer},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2013{\_}Karl Ni, Nicholas Armstrong-Crews{\_}GEO-REGISTERING 3D POINT CLOUDS TO 2D MAPS WITH SCAN MATCHING AND THE HOUGH TRANSFORM.pdf:pdf},
isbn = {9781479903566},
pages = {1864--1868},
title = {{GEO-REGISTERING 3D POINT CLOUDS TO 2D MAPS WITH SCAN MATCHING AND THE HOUGH TRANSFORM}},
year = {2013}
}
@article{Taneja2013,
abstract = {In this paper, we propose a method to detect changes in the geometry of a city using panoramic images captured by a car driving around the city. We designed our approach to account for all the challenges involved in a large scale application of change detection, such as, inaccuracies in the input geometry, errors in the geo-location data of the images, as well as, the limited amount of information due to sparse imagery. We evaluated our approach on an area of 6 square kilometers inside a city, using 3420 images downloaded from Google Street View. These images besides being publicly available, are also a good example of panoramic images captured with a driving vehicle, and hence demonstrating all the possible challenges resulting from such an acquisition. We also quantitatively compared the performance of our approach with respect to a ground truth, as well as to prior work. This evaluation shows that our approach outperforms the current state of the art. View full abstract},
author = {Taneja, Aparna and Ballan, Luca and Pollefeys, Marc},
doi = {10.1109/CVPR.2013.22},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2013{\_}Taneja, Ballan, Pollefeys{\_}City-scale change detection in cadastral 3D models using images.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {3D modeling,Change Detection,Large scale computer vision application},
pages = {113--120},
title = {{City-scale change detection in cadastral 3D models using images}},
year = {2013}
}
@article{Forster2013,
abstract = {We propose a new method for the localization of a Micro Aerial Vehicle (MAV) with respect to a ground robot. We solve the problem of registering the 3D maps computed by the robots using different sensors: a dense 3D reconstruction from the MAV monocular camera is aligned with the map computed from the depth sensor on the ground robot. Once aligned, the dense reconstruction from the MAV is used to augment the map computed by the ground robot, by extending it with the information conveyed by the aerial views. The overall approach is novel, as it builds on recent developments in live dense reconstruction from moving cameras to address the problem of air-ground localization. The core of our contribution is constituted by a novel algorithm integrating dense reconstructions from monocular views, Monte Carlo localization, and an iterative pose refinement. In spite of the radically different vantage points from which the maps are acquired, the proposed method achieves high accuracy whereas appearance-based, state-of-the-art approaches fail. Experimental validation in indoor and outdoor scenarios reported an accuracy in position estimation of 0.08 meters and real time performance. This demonstrates that our new approach effectively overcomes the limitations imposed by the difference in sensors and vantage points that negatively affect previous techniques relying on matching visual features. View full abstract},
author = {Forster, Christian and Pizzoli, Matia and Scaramuzza, Davide},
doi = {10.1109/IROS.2013.6696924},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2013{\_}Forster, Pizzoli, Scaramuzza{\_}Air-ground localization and map augmentation using monocular dense reconstruction.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {3971--3978},
pmid = {6696924},
title = {{Air-ground localization and map augmentation using monocular dense reconstruction}},
year = {2013}
}
@article{Maurer2012,
abstract = {We present an image-based 3D reconstruction pipeline for acquiring geo-referenced semi-dense 3D models. Multiple overlapping images captured from a micro aerial vehicle platform provide a highly redundant source for multi-view reconstructions. Publicly available geo-spatial information sources are used to obtain an approximation to a digital surface model (DSM). Models obtained by the semi-dense reconstruction are automatically aligned to the DSM to allow the integration of highly detailed models into the original DSM and to provide geographic context.},
author = {Maurer, Michael and Rumpler, Markus and Wendel, Andreas and Hoppe, Christof and Irschara, Arnold and Bischof, Horst},
doi = {10.1109/ICRA.2012.6225247},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2012{\_}Maurer et al.{\_}Geo-referenced 3D reconstruction Fusing public geographic data and aerial imagery.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {c},
pages = {3557--3558},
title = {{Geo-referenced 3D reconstruction: Fusing public geographic data and aerial imagery}},
volume = {1},
year = {2012}
}
@article{Rauter2012,
abstract = {In this work we present an efficient GPU implementation of the Fast Directional Chamfer Matching (FDCM) algorithm [10]. We propose some extensions to the original FDCM algorithm. In particular, we extend the algorithm to handle templates with variable size, to account for perspective effects. To the best of our knowledge, our work is the first to present a full implementation of a shape based matching algorithm on a GPU. Further contributions of our work consist of implementing a highly optimized CPU version of the algorithm (via multi-threading and SSE2), as well as a thorough comparison between pure GPU, pure CPU, and a hybrid version. The hybrid CPU-GPU version which turns out to be the fastest, achieves run-time of 44 fps on PAL resolution images.},
author = {Rauter, Michael and Schreiber, David},
doi = {10.1109/CVPRW.2012.6238897},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2012{\_}Rauter, Schreiber{\_}A GPU accelerated Fast Directional Chamfer Matching algorithm and a detailed comparison with a highly optimized C.pdf:pdf},
isbn = {9781467316118},
issn = {21607508},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
number = {May},
pages = {68--75},
title = {{A GPU accelerated Fast Directional Chamfer Matching algorithm and a detailed comparison with a highly optimized CPU implementation}},
year = {2012}
}
@article{Liu2012,
abstract = {The photorealistic modeling of large-scale objects, such as urban scenes, requires the combination of range sensing technology and digital photography. In this paper, we attack the key problem of camera pose estimation, in an automatic and efficient way. First, the camera orientation is recovered by matching vanishing points (extracted from 2D images) with 3D directions (derived from a 3D range model). Then, a hypothesis-and-test algorithm computes the camera positions with respect to the 3D range model by matching corresponding 2D and 3D linear features. The camera positions are further optimized by minimizing a line-to-line distance. The advantage of our method over earlier work has to do with the fact that we do not need to rely on extracted planar facades, or other higher-order features; we are utilizing low-level linear features. That makes this method more general, robust, and efficient. We have also developed a user-interface for allowing users to accurately texture-map 2D images onto 3D range models at interactive rates. We have tested our system in a large variety of urban scenes. ?? 2011 Elsevier Inc. All rights reserved.},
author = {Liu, Lingyun and Stamos, Ioannis},
doi = {10.1016/j.cviu.2011.07.009},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2012{\_}Liu, Stamos{\_}A systematic approach for 2D-image to 3D-range registration in urban environments.pdf:pdf},
isbn = {978-1-4244-1631-8},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {2D-to-3D Registration,Photorealistic 3D modeling},
number = {1},
pages = {25--37},
title = {{A systematic approach for 2D-image to 3D-range registration in urban environments}},
volume = {116},
year = {2012}
}
@article{Pintus2011,
author = {Pintus, Ruggero and Gobbetti, Enrico and Combet, Roberto},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2011{\_}Pintus, Gobbetti, Combet{\_}Fast and Robust Semi-Automatic Registration of Photographs to 3D Geometry.pdf:pdf},
number = {4},
pages = {1--23},
title = {{Fast and Robust Semi-Automatic Registration of Photographs to 3D Geometry}},
volume = {7},
year = {2011}
}
@article{Wendel2011,
abstract = {We present a novel technique for the automatic alignment of Structure from Motion (SfM) models, acquired at ground level or by micro aerial vehicles, to an overhead Digital Surface Model (DSM) using GPS information. An additional refinement step based on the correlation of the DSM height map with the model height map corrects for the GPS localization uncertainties and results in precisely aligned models. Our approach successfully handles cases where previous methods had problems, including objects on the ground, unoccupied space, and models covering a small area. We conclude our work by presenting several applications of our approach, namely the fusion of detailed SfM model information into the original DSM, season-invariant matching using aligned models, and alignment for providing context in visualization.},
author = {Wendel, Andreas and Irschara, Arnold and Bischof, Horst},
doi = {10.1109/CVPRW.2011.5981717},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2011{\_}Wendel, Irschara, Bischof{\_}Automatic alignment of 3D reconstructions using a Digital Surface Model.pdf:pdf},
isbn = {9781457705298},
issn = {21607508},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {29--36},
title = {{Automatic alignment of 3D reconstructions using a Digital Surface Model}},
year = {2011}
}
@article{Cao2011,
abstract = {The complexity of natural scenes and the amount of information acquired by terrestrial laser scanners turn the registration among scans into a complex problem. This problem becomes even more challenging when two individual scans captured at significantly changed viewpoints (wide baseline). Since laser-scanning instruments nowadays are often equipped with an additional image sensor, it stands to reason making use of the image content to improve the registration process of 3D scanning data. In this paper, we present a novel improvement to the existing feature techniques to enable automatic alignment between two widely separated 3D scans. The key idea consists of extracting dominant planar structures from 3D point clouds and then utilizing the recovered 3D geometry to improve the performance of 2D image feature extraction and matching. The resulting features are very discriminative and robust to perspective distortions and viewpoint changes due to exploiting the underlying 3D structure. Using this novel viewpoint invariant feature, the corresponding 3D points are automatically linked in terms of wide baseline image matching. Initial experiments with real data demonstrate the potential of the proposed method for the challenging wide baseline 3D scanning data alignment tasks.},
author = {Cao, Yanpeng and Yang, Michael Ying and McDonald, John},
doi = {10.1109/WACV.2011.5711539},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2011{\_}Cao, Yang, McDonald{\_}Robust alignment of wide baseline terrestrial laser scans via 3D viewpoint normalization.pdf:pdf},
isbn = {9781424494965},
issn = {1550-5790},
journal = {2011 IEEE Workshop on Applications of Computer Vision, WACV 2011},
pages = {455--462},
title = {{Robust alignment of wide baseline terrestrial laser scans via 3D viewpoint normalization}},
year = {2011}
}
@article{FDCM,
author = {Liu, Ming-yu and Tuzel, Oncel},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2010{\_}Liu, Tuzel{\_}Fast Directional Chamfer Matching.pdf:pdf},
isbn = {9781424469857},
journal = {CVPR},
pages = {1696--1703},
title = {{Fast Directional Chamfer Matching}},
year = {2010}
}
@article{Stamos2010,
author = {Stamos, I.},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2010{\_}Stamos{\_}Automated registration of 3D-range with 2D-color Images an overview.pdf:pdf},
isbn = {9781424474172},
journal = {Information Sciences and Systems (CISS)},
number = {figure 1},
pages = {1--6},
title = {{Automated registration of 3D-range with 2D-color Images : an overview}},
volume = {2010 44th},
year = {2010}
}
@article{Wang2009,
abstract = {Airborne LiDAR technology draws increasing interest in large-scale 3D urban modeling in recent years. 3D LiDAR data typically has no texture information. To generate photo-realistic 3D models, oblique aerial images are needed for texture mapping, in which the key step is to obtain accurate registration between aerial images and untextured 3D LiDAR data. We present a robust automatic registration approach. A novel feature called 3CS is proposed which is composed of connected line segments. Putative line segment correspondences are obtained by matching 3CS features detected from both aerial images and 3D LiDAR data. Outliers are removed with a two-level RANSAC algorithm that integrates local and global processing to improve robustness and efficiency. The approach has been tested on 2290 aerial images that cover a variety of urban environments in Oakland and Atlanta areas. Its correct pose recovery rate is over 98{\%}. View full abstract},
author = {Wang, Lu and Neumann, Ulrich},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2009{\_}Wang, Neumann{\_}A robust approach for automatic registration of aerial images with untextured aerial lidar data.pdf:pdf},
isbn = {9781424439911},
journal = {{\ldots} 2009. CVPR 2009. IEEE Conference on},
pages = {2623--2630},
title = {{A robust approach for automatic registration of aerial images with untextured aerial lidar data}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5206600},
year = {2009}
}
@article{Wang2009a,
abstract = {In this paper, we propose an automatic system for robust alignment of 2D optical images with 3D LiDAR (light detection and ranging) data. Focusing on applications such as data fusion and rapid updating for GIS (geographic information systems) from diverse sources when accurate georeference is not available, our goal is a vision-based approach to recover the 2D to 3D transformation including orientation, scale and location. Two major challenges of 2D-3D registration systems are different sensors problem and large 3D viewpoint changes. Due to the challenging nature of the problem, we achieve the registration goal through a two-stage solution. The first stage of the proposed system uses a robust region matching method to handle different sensor problems registering 3D data onto 2D images with similar viewing directions. Robustness to 3D viewpoint changes is achieved by the second stage where 2D views with sensor variations compensated by the first stage are trained. Other views are rapidly matched to them and therefore indirectly registered with 3D LiDAR data. Experimental results using four cities' datasets have illustrated the potential of the proposed system.},
author = {Wang, Quan and You, Suya},
doi = {10.1109/WACV.2009.5403095},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2009{\_}Wang, You{\_}A vision-based 2D-3D registration system.pdf:pdf},
isbn = {9781424454976},
journal = {2009 Workshop on Applications of Computer Vision, WACV 2009},
title = {{A vision-based 2D-3D registration system}},
year = {2009}
}
@article{Li2009,
author = {Li, Yunzhen and Low, Kok-Lim},
doi = {10.1145/1629739.1629742},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2009{\_}Li, Low{\_}Automatic registration of color images to 3D geometry.pdf:pdf},
isbn = {9781605586878},
journal = {Computer Graphics International on - CGI '09},
number = {212},
pages = {21--28},
title = {{Automatic registration of color images to 3D geometry}},
url = {http://portal.acm.org/citation.cfm?doid=1629739.1629742},
volume = {1},
year = {2009}
}
@article{Shaoxing2009,
author = {Shaoxing, Hu and Aiwu, Zhang},
doi = {10.1109/URS.2009.5137584},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2009{\_}Shaoxing, Aiwu{\_}3D laser omnimapping for 3D reconstruction of large-scale scenes.pdf:pdf},
isbn = {9781424434619},
journal = {2009 Joint Urban Remote Sensing Event},
title = {{3D laser omnimapping for 3D reconstruction of large-scale scenes}},
year = {2009}
}
@article{Bileschi2009,
abstract = {This work describes a fully automatic technique to calibrate a geometric mapping between lidar and video feeds on a mobile ground-based platform. This data association is a crucial first step for any multi-modal scene understanding system which aims to leverage the complementary information of the two sensors. While several systems have been previously described which use hand-calibration or specific scenery to achieve this goal, the system described here is fully automatic and generates an accurate association without user intervention or calibration objects. The estimated parameters include the 7 classical camera parameters for a linear pinhole model, i.e., rotation, position, and focal length parameters, as well as an estimation of the radial distortion. The system uses a multi stage process to bootstrap difficult parameters based on robust estimates of easier ones. The calibration algorithm is tested empirically using free online data supplied as part of the DARPA Urban Challenge autonomous vehicle competition. Experiments are performed to illustrate the stability, and computation cost of the algorithm.},
author = {Bileschi, Stanley},
doi = {10.1109/ICCVW.2009.5457439},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2009{\_}Bileschi{\_}Fully automatic calibration of LIDAR and video streams from a vehicle.pdf:pdf},
isbn = {9781424444427},
journal = {2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops 2009},
pages = {1457--1464},
title = {{Fully automatic calibration of LIDAR and video streams from a vehicle}},
year = {2009}
}
@article{Kaminsky2009,
author = {Kaminsky, Ryan S and Snavely, Noah and Seitz, Steven M and Szeliski, Richard},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2009{\_}Kaminsky et al.{\_}Alignment of 3D point clouds to overhead images.pdf:pdf},
isbn = {9781424439935},
journal = {CVPRw},
pages = {63--70},
title = {{Alignment of 3D point clouds to overhead images}},
year = {2009}
}
@article{Zhang2009,
abstract = {Murals are the important components of culture and arts of Tibet. Tibet's murals are mainly painted on the walls of temples, palaces and homes. Unhappily, these murals have been ruined or are being ruined by some diseases such as cracking, hollowing, falling off, fluid erosion, dirt, and so on. It is necessary to use new technologies to protect murals. June 2007, we, from Capital Normal University and National Institute of Cultural Property collected a lot of laser range images and digital images of Tibetpsilas murals. We proposed that integrating range data and image data to investigate mural diseases and give the location, length and area of mural diseases. Moreover, we also presented a parallel computing method to process the huge range data, and discussed to align the images into a big image based on SIFT.},
author = {Zhang, Aiwu and Hu, Shaoxing and Gao, Feng},
doi = {10.1109/IV.2009.109},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2009{\_}Zhang, Hu, Gao{\_}Investigation on diseases of tibet murals using 3D laser scanning technology.pdf:pdf},
isbn = {978-0-7695-3733-7},
issn = {10939547},
journal = {Proceedings of the International Conference on Information Visualisation},
keywords = {3D laser scanning,Mural survey,Range images,SIFT},
number = {Nsfc 40601081},
pages = {568--571},
title = {{Investigation on diseases of tibet murals using 3D laser scanning technology}},
year = {2009}
}
@article{Ding2008,
abstract = {A fast 3D model reconstruction methodology is desirable in many applications such as urban planning, training, and simulations. In this paper, we develop an automated algorithm for texture mapping oblique aerial images onto a 3D model generated from airborne light detection and ranging (LiDAR) data. Our proposed system consists of two steps. In the first step, we combine vanishing points and global positioning system aided inertial system readings to roughly estimate the extrinsic parameters of a calibrated camera. In the second step, we refine the coarse estimate of the first step by applying a series of processing steps. Specifically, We extract 2D corners corresponding to orthogonal 3D structural corners as features from both images and the untextured 3D LiDAR model. The correspondence between an image and the 3D model is then performed using Hough transform and generalized M-estimator sample consensus. The resulting 2D corner matches are used in Lowepsilas algorithm to refine camera parameters obtained earlier. Our system achieves 91{\%} correct pose recovery rate for 90 images over the downtown Berkeley area, and overall 61{\%} accuracy rate for 358 images over the residential, downtown and campus portions of the city of Berkeley.},
author = {Ding, Min and Lyngbaek, Kristian and Zakhor, Avideh},
doi = {10.1109/CVPR.2008.4587661},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2008{\_}Ding, Lyngbaek, Zakhor{\_}Automatic registration of aerial imagery with untextured 3D LiDAR models.pdf:pdf},
isbn = {9781424422432},
issn = {1063-6919},
journal = {26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR},
title = {{Automatic registration of aerial imagery with untextured 3D LiDAR models}},
year = {2008}
}
@article{Pong2006,
abstract = {Mutual information has been used for matching and registering 3D models to 2D images. However, in Viola's original framework [1], surface albedo variance is assumed to be minimal when measuring similarity between 3D models and 2D image data using mutual information. In reality, most objects have textured surfaces with different albedo values across their surfaces, and direct application of this method in such circumstances will fail. To solve this problem, we propose to include spatial information into the original formulation by using histogram-based features of local regions that are robust to local but significant albedo variation. Neighborhood Extended Gaussian Images (NEGI) are used as descriptors to represent local surface regions on the 3D model, while pixel intensity data are considered within corresponding region windows on the image. Experiments on aligning 3D car models in cluttered scenes using this new framework demonstrate substantial improvement as compared to the original pixel-wise mutual information approach.},
author = {Pong, Hon Keat and Cham, Tat Jen},
doi = {10.1007/11612032_7},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2006{\_}Pong, Cham{\_}Alignment of 3D models to images using region-based mutual information and neighborhood extended gaussian images.pdf:pdf},
isbn = {3540312196},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {Mi},
pages = {60--69},
title = {{Alignment of 3D models to images using region-based mutual information and neighborhood extended gaussian images}},
volume = {3851 LNCS},
year = {2006}
}
@article{Liu2006,
author = {Liu, Lingyun and Stamos, Ioannis and Yu, Gene and Wolberg, George and Zokai, Siavash and Technology, Brainstorm},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2016-6-15/2006{\_}Liu et al.{\_}Multiview Geometry for Texture Mapping 2D Images Onto 3D Range Data Computer Vision and Pattern Recognition, 2006.pdf:pdf},
isbn = {0769525970},
journal = {Work},
title = {{Multiview Geometry for Texture Mapping 2D Images Onto 3D Range Data Computer Vision and Pattern Recognition, 2006}},
year = {2006}
}
@article{Zhao2005,
abstract = {We propose a general framework for aligning continuous (oblique) video onto 3D sensor data. We align a point cloud computed from the video onto the point cloud directly obtained from a 3D sensor. This is in contrast to existing techniques where the 2D images are aligned to a 3D model derived from the 3D sensor data. Using point clouds enables the alignment for scenes full of objects that are difficult to model; for example, trees. To compute 3D point clouds from video, motion stereo is used along with a state-of-the-art algorithm for camera pose estimation. Our experiments with real data demonstrate the advantages of the proposed registration algorithm for texturing models in large-scale semiurban environments. The capability to align video before a 3D model is built from the 3D sensor data offers new practical opportunities for 3D modeling. We introduce a novel modeling-through-registration approach that fuses 3D information from both the 3D sensor and the video. Initial experiments with real data illustrate the potential of the proposed approach.},
author = {Zhao, Wenyi and Nister, David and Hsu, Steve},
doi = {10.1109/TPAMI.2005.152},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2005{\_}Zhao, Nister, Hsu{\_}Alignment of continuous video onto 3D point clouds.pdf:pdf},
isbn = {0-7695-2158-4},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {3D model and visualization,Alignment,Motion stereo,Pose estimation,Range data,Sensor fusion},
number = {8},
pages = {1305--1318},
pmid = {16119268},
title = {{Alignment of continuous video onto 3D point clouds}},
volume = {27},
year = {2005}
}
@article{Censi2005,
author = {Censi, Andrea and Iocchi, Luca and Grisetti, Giorgio},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2005{\_}Censi, Iocchi, Grisetti{\_}Scan Matching in the Hough Domain.pdf.pdf:pdf},
isbn = {078038914X},
number = {April},
pages = {39--44},
title = {{Scan Matching in the Hough Domain.pdf}},
year = {2005}
}
@article{Terms2000,
author = {Terms, Read These},
doi = {10.1023/B},
file = {:F$\backslash$:/Hang/papers/ModelMapping/2000{\_}Terms{\_}NRC Publications Archive Archives des publications du CNRC.pdf:pdf},
title = {{NRC Publications Archive Archives des publications du CNRC}},
year = {2000}
}
@article{Hartl2015,
	abstract = {Many security documents contain machine readable zones (MRZ) for automatic inspection. An MRZ is intended to be read by dedicated machinery, which often requires a stationary setup. Although MRZ information can also be read using camera phones, current solutions require the user to align the document, which is rather tedious. We propose a real-time algorithm for MRZ detection and recognition on off-the-shelf mobile devices. In contrast to state-of-the-art solutions, we do not impose position restrictions on the document. Our system can instantly produce robust reading results from a large range of viewpoints, making it suitable for document verification or classification. We evaluate the proposed algorithm using a large synthetic database on a set of off-the-shelf smartphones. The obtained results prove that our solution is capable of achieving good reading accuracy despite using largely unconstrained viewpoints and mobile devices.},
	author = {Hartl, Andreas and Arth, Clemens and Schmalstieg, Dieter},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/visapp15{\_}final.pdf:pdf},
	journal = {Visigrapp},
	pages = {79--87},
	title = {{Real-time Detection and Recognition of Machine-Readable Zones with Mobile Devices}},
	year = {2015}
}
@article{Ventura2015,
	abstract = {We propose an efficient method for estimating the motion of a multi-camera rig from a minimal set of feature correspondences. Existing methods for solving the multi-camera relative pose problem require extra correspondences, are slow to compute, and/or produce a multitude of solutions. Our solution uses a first-order approximation to relative pose in order to simplify the problem and produce an accurate estimate quickly. The solver is applicable to sequential multi-camera motion estimation and is fast enough for real-time implementation in a random sampling framework. Our experiments show that our approach is both stable and efficient on challenging test sequences.},
	author = {Ventura, Jonathan and Arth, Clemens},
	doi = {10.1109/ICCV.2015.92},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/ventura{\_}iccv{\_}2015.pdf:pdf},
	isbn = {978-1-4673-8391-2},
	journal = {International Conference on Computer Vision (ICCV)},
	pages = {4321--4329},
	title = {{An Efficient Minimal Solution for Multi-Camera Motion}},
	url = {https://github.com/jonathanventura/multi-camera-motion},
	year = {2015}
}
@article{Arth2012,
	author = {Arth, Clemens and Reitmayr, Gerhard and Schmalstieg, Dieter},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/Schmalstieg{\_}246.pdf:pdf},
	journal = {Proc. Asian Conference on Computer Vision (ACCV) 2012},
	pages = {705--717},
	title = {{Full 6DOF Pose Estimation from Geo-Located Images}},
	url = {http://data.icg.tugraz.at/{~}dieter/publications/Schmalstieg{\_}246.pdf},
	year = {2012}
}
@article{Hartl2011,
	abstract = {Augmented Reality (AR) on mobile phones is receiving more and more attention recently, becoming a popular research topic and an important commercial field. In this paper we present a lightweight method to create coarse 3D models of small-scale objects. The goal is to give the users the possibility to create and maintain AR content themselves without the need for expensive tools and complex interaction. Our algorithm is based on shape-from-silhouette using voxel carving and runs on modern smartphone hardware. 3D models of certain object groups can be generated interactively and instantly. The actual result is visualized continuously using image based rendering methods to inform the user about the actual model quality. Given a suitably accurate model it can further be used for any means of AR and can easily be shared with other users. The usability of our approach is evaluated using modern smartphone hardware (see Figure 1). The advantages are demonstrated using a set of models for playing a board game.},
	author = {Hartl, Andreas and Gruber, Lukas and Arth, Clemens and Hauswiesner, Stefan and Schmalstieg, Dieter},
	doi = {10.1109/CVPRW.2011.5981789},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/rapid{\_}reconstruction{\_}ecvw11.pdf:pdf},
	isbn = {9781457705298},
	issn = {21607508},
	journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
	title = {{Rapid reconstruction of small objects on mobile phones}},
	year = {2011}
}
@article{Ventura2015a,
	author = {Ventura, Jonathan and Arth, Clemens and Lepetit, Vincent},
	doi = {10.1007/978-3-319-16178-5_12},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/proof.pdf:pdf},
	isbn = {9783319161778},
	issn = {16113349},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	keywords = {Camera pose,Essential matrix,Five-pt algorithm,Gr??bner basis,Relative orientation,Relative pose},
	pages = {180--193},
	title = {{Approximated relative pose solvers for efficient camera motion estimation}},
	volume = {8925},
	year = {2015}
}
@article{Arth2008,
	author = {Arth, Clemens},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/phd{\_}final.pdf:pdf},
	number = {March},
	title = {{Visual surveillance on DSP-based embedded platforms}},
	url = {http://www.icg.tu-graz.ac.at/publications/pdf/arth{\_}phdthesis{\_}2008},
	year = {2008}
}
@article{Arth2009,
	author = {Arth, C and Wagner, D and Klopschitz, M and Irschara, A and Schmalstieg, D},
	doi = {10.1109/ISMAR.2009.5336494},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/paper.pdf:pdf},
	journal = {Proc. 8th IEEE Int. Symp. Mixed and Augmented Reality ISMAR 2009},
	pages = {73--82},
	title = {{Wide area localization on mobile phones}},
	year = {2009}
}
@article{Pan2011,
	abstract = {Rapid 3D reconstruction of environments has become an active research topic due to the importance of 3D models in a huge number of applications, be it in Augmented Reality (AR), architecture or other commercial areas. In this paper we present a novel system that allows for the generation of a coarse 3D model of the environment within several seconds on mobile smartphones. By using a very fast and flexible algorithm a set of panoramic images is captured to form the basis of wide field-of-view images required for reliable and robust reconstruction. A cheap on-line space carving approach based on Delaunay triangulation is employed to obtain dense, polygonal, textured representations. The use of an intuitive method to capture these images, as well as the efficiency of the reconstruction approach allows for an application on recent mobile phone hardware, giving visually pleasing results almost instantly.},
	author = {Pan, Qi and Arth, Clemens and Rosten, Edward and Reitmayr, Gerhard and Drummond, Tom},
	doi = {10.1109/ISMAR.2011.6092370},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/pan{\_}2011{\_}rapid.pdf:pdf},
	isbn = {9781457721830},
	journal = {2011 10th IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2011},
	keywords = {I.2.10 [Artificial Intelligence]: Vision and Scene Understanding-3D/stereo scene analysis I.4.8 [Image Processing And Computer Vision]: Scene Analysis-Tracking,I.5.4 [Pattern Recognition]: Applications-Computer Vision C.5.3 [Computer System Implementation]: Microcomputers-Portable devices (e.g., laptops, personal digital assistants)},
	pages = {55--64},
	title = {{Rapid scene reconstruction on mobile phones from panoramic images}},
	year = {2011}
}
@article{Ventura2014,
	abstract = {We propose the combination of a keyframe-based monocular SLAM system and a global localization method. The SLAM system runs locally on a camera-equipped mobile client and provides continuous, relative 6DoF pose estimation as well as keyframe images with computed camera locations. As the local map expands, a server process localizes the keyframes with a pre-made, globally-registered map and returns the global registration correction to the mobile client. The localization result is updated each time a keyframe is added, and observations of global anchor points are added to the client-side bundle adjustment process to further refine the SLAM map registration and limit drift. The end result is a 6DoF tracking and mapping system which provides globally registered tracking in real-time on a mobile device, overcomes the difficulties of localization with a narrow field-of-view mobile phone camera, and is not limited to tracking only in areas covered by the offline reconstruction.},
	author = {Ventura, Jonathan and Arth, Clemens and Reitmayr, Gerhard and Schmalstieg, Dieter},
	doi = {10.1109/TVCG.2014.27},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/jventura{\_}vr2014.pdf:pdf},
	isbn = {1077-2626 VO  - 20},
	issn = {10772626},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Image-based localization,global positioning,mobile augmented reality,monocular SLAM,real-time tracking},
	number = {4},
	pages = {531--539},
	pmid = {24650980},
	title = {{Global localization from monocular SLAM on a mobile phone}},
	volume = {20},
	year = {2014}
}
@article{Ventura2014a,
	abstract = {We propose a novel solution to the generalized camera pose problem which includes the internal scale of the generalized camera as an unknown parameter. This further generalization of the well-known absolute camera pose problem has applications in multi-frame loop closure. While a well-calibrated camera rig has a fixed and known scale, camera trajectories produced by monocular motion estimation necessarily lack a scale estimate. Thus, when performing loop closure in monocular visual odometry, or registering separate structure-from-motion reconstructions, we must estimate a seven degree-of-freedom similarity transform from corresponding observations. Existing approaches solve this problem, in specialized configurations, by aligning 3D triangulated points or individual camera pose estimates. Our approach handles general configurations of rays and points and directly estimates the full similarity transformation from the 2D-3D correspondences. Four correspondences are needed in the minimal case, which has eight possible solutions. The minimal solver can be used in a hypothesize-and-test architecture for robust transformation estimation. Our solver also produces a least-squares estimate in the overdetermined case. The approach is evaluated experimentally on synthetic and real datasets, and is shown to produce higher accuracy solutions to multi-frame loop closure than existing approaches.},
	author = {Ventura, Jonathan and Arth, Clemens and Reitmayr, Gerhard and Schmalstieg, Dieter},
	doi = {10.1109/CVPR.2014.61},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/jventura{\_}cvpr14.pdf:pdf},
	isbn = {9781479951178},
	issn = {10636919},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	keywords = {3d computer vision,absolute pose,generalized camera,loop closure,minimal solvers,structure from motion},
	pages = {422--429},
	title = {{A minimal solution to the generalized pose-and-scale problem}},
	year = {2014}
}
@article{Hartl2010,
	abstract = {Abstract Segmentation is an integral part of many$\backslash$nmodern image processing systems. It is a hard problem,$\backslash$nwhich is even more tightened in case of mobile setups$\backslash$nbecause of varying conditions in image acquisition$\backslash$nand limited processing power. In this article we describe$\backslash$na framework for robust segmentation, feature extraction$\backslash$nand object recognition which runs in real-time on$\backslash$nmodern mobile phone hardware. Impressive results are$\backslash$npresented as an instant overlay onto the image, which$\backslash$nallows for easy verication by the user. We present a$\backslash$ngeneral evaluation concerning speed and accuracy for$\backslash$nvarious test cases. Our approach is applied in two application$\backslash$nscenarios, namely the recognition of pharmaceutical$\backslash$npills in a mobile use case, and for the task of$\backslash$nAugmented Reality (AR) for entertainment purposes.},
	author = {Hartl, Andreas and Arth, Clemens and Schmalstieg, Dieter},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/iwmv{\_}10.pdf:pdf},
	journal = {Ijvr},
	keywords = {submitted},
	title = {{Real-Time Segmentation and Recognition of Simple Objects on Mobile Phones}},
	year = {2010}
}
@article{Poglitsch2015,
	abstract = {We propose an outdoor localization system using a particle filter. In our approach, a textured, geo-registered model of the outdoor environment is used as a reference to estimate the pose of a smartphone. The device position and the orientation obtained from a Global Positioning System (GPS) receiver and an inertial measurement unit (IMU) are used as a first estimation of the true pose. Then, multiple pose hypotheses are randomly distributed about the GPS/IMU measurement and use to produce renderings of the virtual model. With vision-based methods, the rendered images are compared with the image received from the smartphone, and the matching scores are used to update the particle filter. The outcome of our system improves the camera pose estimate in real time without user assistance.},
	author = {Poglitsch, Christian and Arth, Clemens and Schmalstieg, Dieter and Ventura, Jonathan},
	doi = {10.1109/ISMAR.2015.39},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/ismar15particle.pdf:pdf},
	isbn = {978-1-4673-7660-0},
	journal = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	keywords = {Augmented reality,Cameras,Computational modeling,GPS receiver,Global Positioning System,IMU,Rendering (computer graphics),Sensors,Three-dimensional displays,camera,geo-registered model,global positioning system receiver,image matching,image-based rendering,inertial measurement unit,outdoor localization,outdoor localization system,particle filter approach,particle filtering (numerical methods),pose estimation,rendering (computer graphics),smart phones,smartphone,vision-based method},
	pages = {132--135},
	title = {{[POSTER] A Particle Filter Approach to Outdoor Localization Using Image-Based Rendering}},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7328079},
	year = {2015}
}
@article{Fleck2015,
	abstract = {In this work, we propose a multi-user system for tracking and mapping, which accommodates mobile clients with different capabilities, mediated by a server capable of providing real-time structure from motion. Clients share their observations of the scene according to their individual capabilities. This can involve only keyframe tracking, but also mapping and map densification, if more computational resources are available. Our contribution is a system architecture that lets heterogeneous clients contribute to a collaborative mapping effort, without prescribing fixed capabilities for the client devices. We investigate the implications that the clients' capabilities have on the collaborative reconstruction effort and its use for AR applications.},
	author = {Fleck, P and Arth, C and Pirchheim, C and Schmalstieg, D},
	doi = {10.1109/ISMAR.2015.40},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/ismar15{\_}fleck{\_}multi{\_}user{\_}slam.pdf:pdf},
	isbn = {978-1-4673-7660-0},
	journal = {Mixed and Augmented Reality (ISMAR), 2015 IEEE International Symposium on},
	keywords = {AR applications,Cameras,Collaboration,Mobile handsets,Servers,Simultaneous localization and mapping,Three-dimensional displays,augmented reality,collaborative mapping,collaborative reconstruction effort,groupware,image reconstruction,mobile client swarms,mobile computing,multiuser system,object tracking,tracking},
	pages = {136--139},
	title = {{[POSTER] Tracking and Mapping with a Swarm of Heterogeneous Clients}},
	url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7328080},
	year = {2015}
}
@article{Reinisch2013,
	abstract = {Creating panoramic images in real-time is an expensive operation for mobile devices. Mapping of individual pixels into the panoramic image is the main focus of this paper, since it is one of the most time consuming parts. The pixel-mapping process is transferred from the Central Processing Unit (CPU) to the Graphics Processing Unit (GPU). The independence of pixels being projected allows OpenGL shaders to perform this operation very efficiently. We propose a shader-based mapping approach and confront it with an existing solution. The application is implemented for Android phones and works fluently on current generation devices.},
	author = {Reinisch, Georg and Arth, Clemens and Schmalstieg, Dieter},
	doi = {10.1109/ISMAR.2013.6671810},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/ISMAR13.pdf:pdf},
	isbn = {9781479928699},
	journal = {2013 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2013},
	keywords = {H.5.1 [Information Interfaces and Presentation],Multimedia Information Systems-Artificial, augmented, and virtual realities; Evaluation/methodology},
	pages = {291--292},
	title = {{Panoramic mapping on a mobile phone GPU}},
	year = {2013}
}
@article{Arth2011,
	abstract = {Self-localization in large environments is a vital task for accurately registered information visualization in outdoor Augmented Reality (AR) applications. In this work, we present a system for self-localization on mobile phones using a GPS prior and an online-generated panoramic view of the user's environment. The approach is suitable for executing entirely on current generation mobile devices, such as smartphones. Parallel execution of online incremental panorama generation and accurate 6DOF pose estimation using 3D point reconstructions allows for real-time self-localization and registration in large-scale environments. The power of our approach is demonstrated in several experimental evaluations.},
	author = {Arth, Clemens and Klopschitz, Manfred and Reitmayr, Gerhard and Schmalstieg, Dieter},
	doi = {10.1109/ISMAR.2011.6092368},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/ISMAR11 - Arth - Real-Time Self-Localization from Panoramic Images.pdf:pdf},
	isbn = {9781457721830},
	journal = {2011 10th IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2011},
	pages = {37--46},
	title = {{Real-time self-localization from panoramic images on mobile devices}},
	year = {2011}
}
@article{Arth2012a,
	abstract = {In this paper, we discuss how the sensors available in modern smartphones can improve 6-degree-of-freedom (6DOF) localization in wide-area environments. In our research, we focus on phones as a platform for large- scale Augmented Reality (AR) applications. Thus, our aim is to estimate the position and orientation of the de- vice accurately and fast  it is unrealistic to assume that users are willing to wait tenths of seconds before they can interact with the application. We propose supple- menting vision methods with sensor readings from the compass and accelerometer available in most modern smartphones. We evaluate this approach on a large- scale reconstruction of the city center of Graz, Austria. Our results show that our approach improves both ac- curacy and localization time, in comparison to an ex- isting localization approach based solely on vision. We finally conclude our paper with a real-world validation of the approach on an iPhone 4S.},
	author = {Arth, Clemens and Mulloni, Alessandro and Schmalstieg, Dieter},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/ICPR{\_}final.pdf:pdf},
	isbn = {9784990644116},
	issn = {10514651},
	journal = {Pattern Recognition (ICPR),},
	number = {Icpr},
	pages = {2152--2156},
	title = {{Exploiting Sensors on Mobile Phones to Improve Wide-Area Localization}},
	url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6460588},
	year = {2012}
}
@article{Hartl2011a,
	author = {Hartl, Andreas and Arth, Clemens and Schmalstieg, Dieter},
	doi = {10.2316/P.2011.740-017},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/IASTED10.pdf:pdf},
	isbn = {978-0-88986-874-8},
	journal = {International Conference on Computer Vision (ICCV)},
	keywords = {mobile computer vision-,object recognition},
	pages = {188--195},
	title = {{Instant Medical Pill Recognition on Mobile Phones}},
	url = {http://www.actapress.com/PaperInfo.aspx?paperId=451961},
	year = {2011}
}
@article{Hartl2015a,
	author = {Hartl, Andreas and Grubert, Jens and Reinbacher, Christian and Arth, Clemens and Schmalstiegp, Dieter},
	doi = {10.1109/VR.2015.7223333},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/holograms{\_}vr{\_}2015{\_}author{\_}version.pdf:pdf},
	isbn = {9781479917273},
	journal = {2015 IEEE Virtual Reality Conference, VR 2015 - Proceedings},
	keywords = {Document inspection,augmented reality,holograms,mobile devices,user interfaces},
	pages = {119--126},
	title = {{Mobile user interfaces for efficient verification of holograms}},
	year = {2015}
}
@article{Arth2015,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1505.01319v2},
	author = {Arth, Clemens and Grasset, Raphael and Gruber, Lukas and Langlotz, Tobias and Mulloni, Alessandro and Wagner, Daniel},
	eprint = {arXiv:1505.01319v2},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/historyAR.pdf:pdf},
	journal = {Computer Graphics and Vision},
	pages = {1--40},
	title = {{The History of Mobile Augmented Reality}},
	volume = {2},
	year = {2015}
}
@article{Hartl2014,
	author = {Hartl, Andreas and Arth, Clemens and Schmalstieg, Dieter},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/hartl{\_}isvc14.pdf:pdf},
	isbn = {9783319143637},
	issn = {16113349},
	journal = {International Symposium on Visual Computing - (ISVC'14)},
	number = {Cv},
	pages = {335--346},
	title = {{AR-based Hologram Detection on Security Documents using a Mobile Phone}},
	year = {2014}
}
@article{Hartl2015b,
	abstract = {Although holograms provide a reasonable level of security, for lay-people, the inspection is difficult due to the inherent lack of knowledge. Mobile AR setups using off-the-shelf smartphones can provide this information and support the verification process. However, current approaches lack in accuracy or are difficult to instruct to the user. We propose a series of designs for holograms suitable for mobile verification. They support the orthogonal sampling of elements within a constrained space along with an assessment of the capture conditions. We evaluated these designs regarding their verification performance and task completion time. The associated study revealed encouraging results for robust and efficient hologram verification on mobiles using a specially designed security element.},
	author = {Hartl, Andreas Daniel and Isop, Werner Alexander and Arth, Clemens and Schmalstieg, Dieter},
	doi = {10.1109/ISMARW.2015.22},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/hartl{\_}hologram-design{\_}ismar15ws.pdf:pdf},
	isbn = {9781467384711},
	journal = {Proceedings of the 2015 IEEE International Symposium on Mixed and Augmented Reality Workshops, ISMARW 2015},
	keywords = {Augmented,H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial},
	pages = {75--80},
	title = {{Towards mobile recognition and verification of holograms using orthogonal sampling}},
	year = {2015}
}
@article{Arth2013,
	author = {Arth, Clemens and Ventura, Jonathan and Schmalstieg, Dieter},
	doi = {10.1109/COMGEO.2013.10},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/geo2013{\_}final.pdf:pdf},
	isbn = {9780769550121},
	journal = {Proceedings - 2013 4th International Conference on Computing for Geospatial Research and Application, COM.Geo 2013},
	keywords = {Augmented Reality,Computer Vision,Structure-from-Motion},
	pages = {64--69},
	title = {{Geospatial management and utilization of large-scale urban visual reconstructions}},
	year = {2013}
}
@article{Arth2015a,
	abstract = {We propose a method for estimating the 3D pose for the camera of a mobile device in outdoor conditions, using only an untextured 2D model. Previous methods compute only a relative pose using a SLAM algorithm, or require many registered images, which are cumbersome to acquire. By contrast, our method returns an accurate, absolute cam- era pose in an absolute referential using simple 2D+height maps, which are broadly available, to re fine a first esti- mate ofthe pose provided by the device's sensors. We show how to first estimate the camera absolute orientation from straight line segments, and then how to estimate the trans- lation by aligning the 2D map with a semantic segmentation of the input image. We demonstrate the robustness and ac- curacy of our approach on achallenging dataset.},
	author = {Arth, C and Pirchheim, C and Lepetit, V and Ventura, J},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/cvpr15submission.pdf:pdf},
	journal = {arXiv preprint arXiv:1503.02675},
	title = {{Global 6DOF Pose Estimation from Untextured 2D City Models}},
	url = {http://arxiv.org/abs/1503.02675},
	year = {2015}
}
@article{Reinisch2013a,
	abstract = {Creating panoramic images in real-time is an expensive operation for mobile devices. Mapping of individual pixels into the panoramic image is the main focus of this paper, since it is one of the most time consuming parts. The pixel-mapping process is transferred from the Central Processing Unit (CPU) to the Graphics Processing Unit (GPU). The independence of pixels being projected allows OpenGL shaders to perform this operation very efficiently. We propose a shader-based mapping approach and confront it with an existing solution. The application is implemented for Android phones and works fluently on current generation devices.},
	author = {Reinisch, Georg and Arth, Clemens and Schmalstieg, Dieter},
	doi = {10.1109/ISMAR.2013.6671810},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/cescgsmpl.pdf:pdf},
	isbn = {9781479928699},
	journal = {2013 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2013},
	keywords = {H.5.1 [Information Interfaces and Presentation],Multimedia Information Systems-Artificial, augmented, and virtual realities; Evaluation/methodology},
	pages = {291--292},
	title = {{Panoramic mapping on a mobile phone GPU}},
	year = {2013}
}
@article{Arth2011a,
	author = {Arth, Clemens and Schmalstieg, Dieter},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/challenges.pdf:pdf},
	journal = {ISMAR 2011 Workshop},
	pages = {1--4},
	title = {{Challenges of Large-Scale Augmented Reality on Smartphones}},
	year = {2011}
}
@article{instant,
	abstract = {We present a method for large-scale geo-localization and global tracking of mobile devices in urban outdoor environments. In contrast to existing methods, we instantaneously initialize and globally register a SLAM map by localizing the first keyframe with respect to widely available untextured 2.5D maps. Given a single image frame and a coarse sensor pose prior, our localization method estimates the absolute camera orientation from straight line segments and the translation by aligning the city map model with a semantic segmentation of the image. We use the resulting 6DOF pose, together with information inferred from the city map model, to reliably initialize and extend a 3D SLAM map in a global coordinate system, applying a model-supported SLAM mapping approach. We show the robustness and accuracy of our localization approach on a challenging dataset, and demonstrate unconstrained global SLAM mapping and tracking of arbitrary camera motion on several sequences.},
	author = {Arth, Clemens and Pirchheim, Christian and Ventura, Jonathan and Schmalstieg, Dieter and Lepetit, Vincent},
	doi = {10.1109/TVCG.2015.2459772},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/arth{\_}ismar15{\_}tvcg{\_}ieeetran.pdf:pdf},
	issn = {10772626},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	keywords = {2D map,SLAM,geo-localization,image registration,outdoor augmented reality},
	number = {11},
	pages = {1309--1318},
	pmid = {26340773},
	title = {{Instant Outdoor Localization and SLAM Initialization from 2.5D Maps}},
	volume = {21},
	year = {2015}
}
@article{Hartl2015c,
	author = {Hartl, Andreas and Arth, Clemens and Grubert, Jens and Schmalstieg, Dieter},
	doi = {10.1109/TVCG.2015.2498612},
	file = {:F$\backslash$:/Hang/papers/Clemens Arth/07321828.pdf:pdf},
	issn = {1077-2626},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	number = {c},
	pages = {1--1},
	title = {{Efficient Verification of Holograms Using Mobile Augmented Reality}},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7321828},
	volume = {2626},
	year = {2015}
}
